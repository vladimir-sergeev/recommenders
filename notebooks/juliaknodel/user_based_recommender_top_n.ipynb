{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Класс реализованного метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedRecommender():\n",
    "    def __init__(self, path_to_orders, path_to_order_products_train, decomposition_dim):\n",
    "        '''\n",
    "        :param path_to_orders: путь к исходному датафрейму с keggle orders.csv\n",
    "        :param path_to_order_products_train: путь к новому тренировочному датафрейму, полученному в п.II ниже\n",
    "        :param decomposition_dim: размерность декомпозиции\n",
    "        '''\n",
    "        self.path_to_orders = path_to_orders\n",
    "        self.path_to_order_products_train = path_to_order_products_train\n",
    "        self.n_comp = decomposition_dim\n",
    "        \n",
    "        self.ok_status = True\n",
    "        check, message = self.check_input_params()\n",
    "        \n",
    "        if not check:\n",
    "            self.ok_status = False\n",
    "            print(message)\n",
    "    \n",
    "    def check_input_params(self):\n",
    "        if not os.path.isfile(self.path_to_orders):\n",
    "            return False, 'File doesnt exist'\n",
    "        if not os.path.isfile(self.path_to_order_products_train):\n",
    "            return False, 'File doesnt exist'\n",
    "        if not isinstance(self.n_comp, int) or not self.n_comp > 0:\n",
    "            return False, 'decomposition_dim should be positive integer value'\n",
    "        \n",
    "        return True, 'Ok'\n",
    "            \n",
    "    def read_data(self):\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.orders = pd.read_csv(self.path_to_orders)\n",
    "        self.order_products_train = pd.read_csv(self.path_to_order_products_train)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        '''\n",
    "        создает таблицу user-item для дальнейших манипуляций\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        merged = pd.merge(self.orders, self.order_products_train, on='order_id', how='right')[['order_id', 'user_id', 'product_id']]\n",
    "        # merged = self.merged[['order_id', 'user_id', 'product_id']]\n",
    "        \n",
    "        self.user_id_num = {key: value for value, key in enumerate(merged['user_id'].unique(), start=0)}\n",
    "        self.product_id_num = {key: value for value, key in enumerate(merged['product_id'].unique(), start=0)}\n",
    "        self.product_num_id = {value: key for value, key in enumerate(merged['product_id'].unique(), start=0)}\n",
    "        \n",
    "        merged['user_num'] = merged['user_id'].transform(lambda x: self.user_id_num[x])\n",
    "        merged['product_num'] = merged['product_id'].transform(lambda x: self.product_id_num[x])\n",
    "        merged['buy'] = 1\n",
    "        \n",
    "        # матрица купил/не купил\n",
    "        self.user_item_matrix = sps.coo_matrix((merged.buy, (merged.user_num, merged.product_num)))\n",
    "        \n",
    "        merged = merged.groupby(by=['user_num', 'product_num']).count()['product_id']\n",
    "        \n",
    "        self.user_item_count_matrix = sps.coo_matrix((merged.values, (merged.index.get_level_values(0).values, \\\n",
    "                                                                      merged.index.get_level_values(1).values)))\n",
    "        \n",
    "    def decompose(self):\n",
    "        '''\n",
    "        декомпозиция user-item_matrix с размерностью n_comp\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.model = NMF(n_components=self.n_comp, init='random', random_state=0)\n",
    "        self.user_matrix = self.model.fit_transform(self.user_item_matrix)\n",
    "        self.product_matrix = self.model.components_\n",
    "    \n",
    "    def get_similarity_by_users_list(self, top_n_size, users):\n",
    "        '''Каждому юзеру сопоставляет id n-схожих юзеров\n",
    "        :param top_n_size: сколько схожих пользователей по убыванию схожести надо сохранить\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        \n",
    "        self.similar_users_df = pd.DataFrame()\n",
    "        self.similar_users_weight_df = pd.DataFrame()\n",
    "\n",
    "        for user_id in tqdm(users):\n",
    "            start = self.user_id_num[user_id]\n",
    "            end = start + 1\n",
    "                \n",
    "            user_to_others_similarity = cosine_similarity(Y=self.user_matrix, X=self.user_matrix[start:end])\n",
    "                \n",
    "            sim_users_df = pd.DataFrame(np.argsort(-user_to_others_similarity)).loc[:, :top_n_size]\n",
    "            sim_users_weight_df = pd.DataFrame(-np.sort(-user_to_others_similarity)).loc[:, :top_n_size]\n",
    "                \n",
    "            self.similar_users_df = pd.concat([self.similar_users_df, sim_users_df], ignore_index=True)\n",
    "            self.similar_users_weight_df = pd.concat([self.similar_users_weight_df, sim_users_weight_df], ignore_index=True)\n",
    "            \n",
    "    def get_similarity(self, top_n_size):\n",
    "        '''Каждому юзеру сопоставляет id n-схожих юзеров\n",
    "        :param top_n_size: сколько схожих пользователей по убыванию схожести надо сохранить\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        \n",
    "        self.similar_users_df = pd.DataFrame()\n",
    "        self.similar_users_weight_df = pd.DataFrame()\n",
    "            \n",
    "        size = 100\n",
    "        users_cnt = len(self.user_matrix)\n",
    "        add = 1 if users_cnt % size > 0 else 0\n",
    "        parts_cnt = users_cnt//size + add\n",
    "\n",
    "        for i in tqdm(range(parts_cnt)):\n",
    "            start = size * i\n",
    "            end = size * i + size\n",
    "                \n",
    "            user_to_others_similarity = cosine_similarity(Y=self.user_matrix, X=self.user_matrix[start:end])\n",
    "                \n",
    "            sim_users_df = pd.DataFrame(np.argsort(-user_to_others_similarity)).loc[:, :top_n_size]\n",
    "            sim_users_weight_df = pd.DataFrame(-np.sort(-user_to_others_similarity)).loc[:, :top_n_size]\n",
    "                \n",
    "            self.similar_users_df = pd.concat([self.similar_users_df, sim_users_df], ignore_index=True)\n",
    "            self.similar_users_weight_df = pd.concat([self.similar_users_weight_df, sim_users_weight_df], ignore_index=True)\n",
    "    \n",
    "    def save_similarity_df(self, path_users, path_weights):\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.similar_users_df.to_csv(path_users, index=False)\n",
    "        self.similar_users_weight_df.to_csv(path_weights, index=False)\n",
    "        \n",
    "    def set_similarity_df(self, path_to_df_users, path_to_df_weights):\n",
    "        '''\n",
    "        Если уже есть посчитанная матрица, то можем использовать и обойти долгий кусок обучения в fit(),\n",
    "        указав параметр loaded_df=True\n",
    "        :param path_to_df: путь до csv файла с учебным датафреймом\n",
    "        '''\n",
    "        if os.path.isfile(path_to_df_users) and os.path.isfile(path_to_df_weights):\n",
    "            self.similar_users_df = pd.read_csv(path_to_df_users)\n",
    "            self.similar_users_weight_df = pd.read_csv(path_to_df_weights)\n",
    "        else:\n",
    "            # оформить ошибки по-человечески - нужен же возврат к True\n",
    "            # self.ok_status = False\n",
    "            print('File doesnt exist')\n",
    "    \n",
    "    def fit(self, path_to_save_users=False, path_to_save_weights=False, loaded_df=False, top_n_size=5, users_list=False):\n",
    "        '''\n",
    "        :param self:\n",
    "        :param path_to_save: путь для сохранения матрицы сх. юзеров и весов (названия дефолтные)\n",
    "        :param loaded_df: bool - True если матрица уже была загружена и пересчитывать заново ее не нужно;\n",
    "                          default=False\n",
    "        :param top_n_size: размер топ схожих пользователей \n",
    "        :return: информация в случае ошибки \n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.top_n_size = top_n_size\n",
    "        \n",
    "        self.read_data()     # читает необходимые df\n",
    "        print('data readed')\n",
    "        self.prepare_data()  # создание таблицы user-item с необх. полями и разметками\n",
    "        print('data prepared')\n",
    "        if not loaded_df:\n",
    "            self.decompose()     # декомпозиция матрицы на 2 матрицы размерности n\n",
    "            print('decomposition done')\n",
    "            if users_list:\n",
    "                self.get_similarity_by_users_list(self.top_n_size, users_list)\n",
    "            else:\n",
    "                self.get_similarity(self.top_n_size)\n",
    "            print('similarity matrix done')\n",
    "            if path_to_save_users and path_to_save_weights:\n",
    "                self.save_similarity_df(path_to_save_users, path_to_save_weights)\n",
    "                print('data saved')\n",
    "    \n",
    "    def predict(self, user_id=False, sim_users_count=False, users_count=False):\n",
    "        '''\n",
    "        :param self:\n",
    "        :param user_id:  id юзера, для которого считаем предсказание покупок, если False, то считаем всех\n",
    "        :param sim_users_count: если нужно учесть не весь топ схожих юзеров, а только его часть, \n",
    "                                равную значению данного параметра\n",
    "        :param users_count: если нужно посчитать не для одного/всех юзеров, а для части, \n",
    "                                равной значению данного параметра\n",
    "        :return: сообщение в случае ошибки; в случае успеха - словарь {user_id: products_id}\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        if not sim_users_count or sim_users_count > self.top_n_size:\n",
    "            sim_users_count = self.top_n_size\n",
    "        \n",
    "        if user_id:\n",
    "            if user_id in self.user_id_num:\n",
    "                users_to_check = [user_id] \n",
    "            else:\n",
    "                return 'There is no user with this id'\n",
    "        else:\n",
    "            users_to_check = [*self.user_id_num][:users_count] if users_count else [*self.user_id_num]\n",
    "            \n",
    "        self.predicted_products = dict()\n",
    "            \n",
    "        users_purchases = pd.Series.sparse.from_coo(self.user_item_matrix)\n",
    "        \n",
    "\n",
    "        \n",
    "        # проход по всем юзерам, для которых хотим получить предсказание покупок\n",
    "        for user_id in tqdm(users_to_check):\n",
    "            user_num = self.get_user_num(user_id)\n",
    "            \n",
    "            # номера и веса схожих c user_id пользователей\n",
    "            sim_users_nums = self.similar_users_df.loc[user_num].values\n",
    "            sim_users_weights = self.similar_users_weight_df.loc[user_num].values\n",
    "\n",
    "            d = defaultdict(int)\n",
    "            \n",
    "            # проход по всем юзерам схожим с предсказываемым\n",
    "            for sim_user_num, sim_user_weight in zip(sim_users_nums[:sim_users_count], sim_users_weights[:sim_users_count]):\n",
    "            \n",
    "                sim_user_purchases = users_purchases[sim_user_num].index\n",
    "                \n",
    "                # к каждому купленному схожим юзером продукту прибавляем его вес относительно предсказываемого\n",
    "                for product in sim_user_purchases:\n",
    "                        d[product] += sim_user_weight\n",
    "\n",
    "            d_ids = defaultdict(int)\n",
    "\n",
    "            # ключами были номера продуктов - заменим на id\n",
    "            for key in d:\n",
    "                d_ids[self.product_num_id[key]] = d[key]\n",
    "                \n",
    "            self.predicted_products[user_id] = [*dict(sorted(d_ids.items(), key=lambda x: x[1], reverse=True))]\n",
    "        return self.predicted_products\n",
    "    \n",
    "    def predict_by_count(self, user_id=False, sim_users_count=False, users_count=False, check_users=False, products_top_size=10):\n",
    "        '''\n",
    "        отличается от predict тем, что учитывает кол-во купленного и рассматривает у каждого схожего юзера только top-n продуктов\n",
    "        \n",
    "        :param self:\n",
    "        :param user_id:  id юзера, для которого считаем предсказание покупок, если False, то считаем всех\n",
    "        :param sim_users_count: если нужно учесть не весь топ схожих юзеров, а только его часть, \n",
    "                                равную значению данного параметра\n",
    "        :param users_count: если нужно посчитать не для одного/всех юзеров, а для части, \n",
    "                                равной значению данного параметра\n",
    "        :return: сообщение в случае ошибки; в случае успеха - словарь {user_id: products_id}\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        if not sim_users_count or sim_users_count > self.top_n_size:\n",
    "            sim_users_count = self.top_n_size\n",
    "        \n",
    "        if user_id:\n",
    "            if user_id in self.user_id_num:\n",
    "                users_to_check = [user_id] \n",
    "            else:\n",
    "                return 'There is no user with this id'\n",
    "        elif check_users:\n",
    "            users_to_check = check_users\n",
    "            users_nums_in_df = {key: value for value, key in enumerate(check_users, start=0)}\n",
    "        else:\n",
    "            users_to_check = [*self.user_id_num][:users_count] if users_count else [*self.user_id_num]\n",
    "            \n",
    "        self.predicted_products = dict()\n",
    "            \n",
    "        users_purchases = pd.Series.sparse.from_coo(self.user_item_count_matrix)\n",
    "        \n",
    "        # проход по всем юзерам, для которых хотим получить предсказание покупок\n",
    "        for user_id in tqdm(users_to_check):\n",
    "            user_num = self.get_user_num(user_id)\n",
    "            \n",
    "            user_num_in_df = user_num\n",
    "            if check_users:\n",
    "                user_num_in_df = users_nums_in_df[user_id]\n",
    "                \n",
    "            # номера и веса схожих c user_id пользователей\n",
    "            sim_users_nums = self.similar_users_df.loc[user_num_in_df].values\n",
    "            sim_users_weights = self.similar_users_weight_df.loc[user_num_in_df].values\n",
    "\n",
    "            d = defaultdict(int)\n",
    "            \n",
    "            # проход по всем юзерам схожим с предсказываемым\n",
    "            for sim_user_num, sim_user_weight in zip(sim_users_nums[:sim_users_count], sim_users_weights[:sim_users_count]):\n",
    "                \n",
    "                if sim_user_num == user_num:\n",
    "                    continue\n",
    "                \n",
    "                sim_user_purchases = dict(users_purchases[sim_user_num])\n",
    "                \n",
    "                # к каждому продукта из топ-n купленных схожим юзером прибавляем его вес относительно предсказываемого\n",
    "                for product, count in sorted(sim_user_purchases.items(), key=lambda x: x[1], reverse=True)[:products_top_size]:\n",
    "                        d[product] += sim_user_weight\n",
    "\n",
    "            d_ids = defaultdict(int)\n",
    "\n",
    "            # ключами были номера продуктов - заменим на id\n",
    "            for key in d:\n",
    "                d_ids[self.product_num_id[key]] = d[key]\n",
    "                \n",
    "            self.predicted_products[user_id] = [*dict(sorted(d_ids.items(), key=lambda x: x[1], reverse=True))]\n",
    "        return self.predicted_products\n",
    "    \n",
    "    def get_coo(self):\n",
    "        return self.user_item_matrix\n",
    "    \n",
    "    def get_user_num(self, user_id):\n",
    "        return self.user_id_num[user_id]\n",
    "    \n",
    "    def prepare_validation_data(self, path_to_valid_df):\n",
    "        '''\n",
    "        создает таблицу user-item для дальнейших манипуляций\n",
    "        :param path_to_valid_df: путь до csv таблицы с валидационным датафреймом\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.order_products_valid = pd.read_csv(path_to_valid_df)\n",
    "        \n",
    "        self.merged_valid = pd.merge(self.orders, self.order_products_valid, on='order_id', how='right')\n",
    "        self.merged_valid = self.merged_valid[['order_id', 'user_id', 'product_id']]\n",
    "        \n",
    "        self.user_id_num_valid = {key: value for value, key in enumerate(self.merged_valid['user_id'].unique(), start=0)}\n",
    "        self.product_id_num_valid = {key: value for value, key in enumerate(self.merged_valid['product_id'].unique(), start=0)}\n",
    "        self.product_num_id_valid = {value: key for value, key in enumerate(self.merged_valid['product_id'].unique(), start=0)}\n",
    "        \n",
    "        self.merged_valid['user_num'] = self.merged_valid['user_id'].transform(lambda x: self.user_id_num_valid[x])\n",
    "        self.merged_valid['product_num'] = self.merged_valid['product_id'].transform(lambda x: self.product_id_num_valid[x])\n",
    "        self.merged_valid['buy'] = 1\n",
    "        \n",
    "        self.user_item_matrix_valid = sps.coo_matrix((self.merged_valid.buy, (self.merged_valid.user_num, self.merged_valid.product_num)))\n",
    "        \n",
    "    def get_data_to_check_metrics(self, users_count=False, sim_users_count=False, products_top_size=10):\n",
    "        '''\n",
    "        :param count_users: для скольких юзеров посчитаем метрики\n",
    "        :param sim_users_count: сколько юзеров из топа схожих будут учитываться для предсказания покупок\n",
    "        '''\n",
    "        predicted_purchases = {}\n",
    "        actual_purchases = {}\n",
    "        \n",
    "        # predicted = self.predict(sim_users_count=sim_users_count, users_count=users_count)\n",
    "        predicted = self.predict_by_count(sim_users_count=sim_users_count, users_count=users_count, products_top_size=products_top_size)\n",
    "        \n",
    "        users_purchases_valid = pd.Series.sparse.from_coo(self.user_item_matrix_valid)\n",
    "        \n",
    "        for user_id in predicted:\n",
    "            if user_id in self.user_id_num_valid:\n",
    "                user_num_in_valid_df = self.user_id_num_valid[user_id]\n",
    "                user_actual_purchases_nums = users_purchases_valid[user_num_in_valid_df].index\n",
    "                    \n",
    "                actual = [self.product_num_id_valid[product_num] for product_num in user_actual_purchases_nums]\n",
    "            \n",
    "                predicted_purchases[user_id] = predicted[user_id]\n",
    "                actual_purchases[user_id] = actual\n",
    "            \n",
    "            else:\n",
    "                print('No user with id: ', user_id)\n",
    "        \n",
    "        return actual_purchases, predicted_purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Пропишем все пути и сконструируем датасеты мечты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# необходимые файлы с исходными датасетами с keggle\n",
    "path_data_train = 'D:/Projects/Anaconda/EDA/data/order_products__train.csv'\n",
    "path_data_prior = 'D:/Projects/Anaconda/EDA/data/order_products__prior.csv'\n",
    "path_data_orders = 'D:/Projects/Anaconda/EDA/data/orders.csv'\n",
    "    \n",
    "# [куда запишем]/[где уже лежат] файлы с индексами\n",
    "train_path = 'data_split/train_indices.pickle'\n",
    "valid_path = 'data_split/validation_indices.pickle'\n",
    "test_path = 'data_split/test_indices.pickle'\n",
    "\n",
    "# куда запишем файлы с новым разделением\n",
    "# df_train_path = 'data_split/train_data.csv'\n",
    "# df_valid_path = 'data_split/validation_data.csv'\n",
    "# df_test_path = 'data_split/test_data.csv'\n",
    "\n",
    "# куда запишем файлы с новым разделением\n",
    "df_train_path = 'data_split/train_1_data.csv'\n",
    "df_valid_path = 'data_split/validation_1_data.csv'\n",
    "df_test_path = 'data_split/test_1_data.csv'\n",
    "df_train_valid = 'data_split/merged_train_valid_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(df_train_path)\n",
    "df_valid = pd.read_csv(df_valid_path)\n",
    "    \n",
    "df_train_valid = pd.concat([df_train, df_valid])\n",
    "\n",
    "df_train_valid.to_csv('data_split/merged_train_valid_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Если файлов с индексами нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_indices(grouped_ratings, retriever):\n",
    "    return np.concatenate(grouped_ratings.apply(retriever).values)\n",
    "\n",
    "def split(path_to_orders):\n",
    "    orders = pd.read_csv(path_to_orders)\n",
    "    grouper = orders.sort_values('order_number').groupby('user_id')\n",
    "    train_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings[:int(user_ratings.shape[0] * 0.5)].index.values)\n",
    "    \n",
    "    validation_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings.iloc[int(user_ratings.shape[0] * 0.5):\n",
    "                                               int(user_ratings.shape[0] * 0.75)].index.values)\n",
    "    \n",
    "    test_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings.iloc[int(user_ratings.shape[0] * 0.75):].index.values)\n",
    "    \n",
    "    return train_indices, validation_indices, test_indices\n",
    "\n",
    "\n",
    "train_indices, validation_indices, test_indices = split(path_data_orders)\n",
    "\n",
    "# save results\n",
    "with open(train_path, 'wb') as out:\n",
    "    pickle.dump(train_indices, out)\n",
    "\n",
    "with open(valid_path, 'wb') as out:\n",
    "    pickle.dump(validation_indices, out)\n",
    "\n",
    "with open(test_path, 'wb') as out:\n",
    "    pickle.dump(test_indices, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файлы с индексами готовы - теперь скомпонуем и запишем в файл все необходимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concat_same_data(df_train_path, df_prior_path, df_orders_path):\n",
    "    \n",
    "    df_train = pd.read_csv(df_train_path)\n",
    "    df_prior = pd.read_csv(df_prior_path)\n",
    "    df_orders = pd.read_csv(df_orders_path)\n",
    "    \n",
    "    order_products_full = pd.concat([df_train, df_prior])\n",
    "    \n",
    "    return order_products_full, df_orders\n",
    "\n",
    "\n",
    "def get_split_df(train_path, valid_path, test_path, df_orders, df_order_products_full):\n",
    "    \n",
    "    with open(train_path, 'rb') as input:\n",
    "        train_indices = pickle.load(input)\n",
    "\n",
    "    with open(valid_path, 'rb') as input:\n",
    "        validation_indices = pickle.load(input)\n",
    "\n",
    "    with open(test_path, 'rb') as input:\n",
    "        test_indices = pickle.load(input)\n",
    "        \n",
    "    train_order_ids = df_orders.loc[train_indices]['order_id'].values\n",
    "    validation_order_ids = df_orders.loc[validation_indices]['order_id'].values\n",
    "    test_order_ids = df_orders.loc[test_indices]['order_id'].values\n",
    "    \n",
    "    train_df = df_order_products_full[df_order_products_full['order_id'].isin(train_order_ids)]\n",
    "    validation_df = df_order_products_full[df_order_products_full['order_id'].isin(validation_order_ids)]\n",
    "    test_df = df_order_products_full[df_order_products_full['order_id'].isin(test_order_ids)]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "# считается что уже имеем 3 файла с индексами\n",
    "df_order_products_full, df_orders = read_and_concat_same_data(path_data_train, path_data_prior, path_data_orders)\n",
    "train_df, validation_df, test_df = get_split_df(train_path, valid_path, test_path, df_orders, df_order_products_full)\n",
    "\n",
    "# write new df's to file:\n",
    "train_df.to_csv(df_train_path, index=False)\n",
    "validation_df.to_csv(df_valid_path, index=False)\n",
    "test_df.to_csv(df_test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Посчитаем с новым алгоритмом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('users_subsample.pickle', 'rb') as input:\n",
    "        user_list = pickle.load(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data readed\n",
      "data prepared\n",
      "decomposition done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:84: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db439661ca9848dba8015403005545c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarity matrix done\n",
      "data saved\n"
     ]
    }
   ],
   "source": [
    "# прекрасный пример для подсчета по одному. но из определенного списка id, поправила параметр в fit - users_list, \n",
    "# надо просто его убрать, чотбы считалось все\n",
    "\n",
    "with open('users_subsample.pickle', 'rb') as input:\n",
    "        user_list = pickle.load(input)\n",
    "\n",
    "# файл, куда по необходимости загрузим результаты\n",
    "path_to_res_users = 'data/train_valid_res_top_100_dim_35_users.csv'\n",
    "path_to_res_weights = 'data/train_valid_res_top_100_dim_35_weights.csv'\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_valid_path = 'data_split/merged_train_valid_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 35\n",
    "\n",
    "# top_n\n",
    "top_n_size = 100\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_valid_path, n_comp)\n",
    "\n",
    "Recommender.fit(path_to_save_users=path_to_res_users, path_to_save_weights=path_to_res_weights, top_n_size=top_n_size, users_list=user_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прекрасный пример для подсчета по одному. но из определенного списка id, поправила параметр в fit - users_list, \n",
    "# надо просто его убрать, чотбы считалось все\n",
    "\n",
    "with open('users_subsample.pickle', 'rb') as input:\n",
    "        user_list = pickle.load(input)\n",
    "\n",
    "# файл, куда по необходимости загрузим результаты\n",
    "path_to_res_users = 'data/train_valid_res_top_100_dim_100_users.csv'\n",
    "path_to_res_weights = 'data/train_valid_res_top_100_dim_100_weights.csv'\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_valid_path = 'data_split/merged_train_valid_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 100\n",
    "\n",
    "# top_n\n",
    "top_n_size = 100\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_valid_path, n_comp)\n",
    "\n",
    "Recommender.fit(path_to_save_users=path_to_res_users, path_to_save_weights=path_to_res_weights, top_n_size=top_n_size, users_list=user_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример без обучения (просто загрузка рез-тов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data readed\n",
      "data prepared\n"
     ]
    }
   ],
   "source": [
    "# файл, куда по необходимости загрузим результаты\n",
    "path_to_res_users = 'data/train_valid_res_top_100_dim_35_users.csv'\n",
    "path_to_res_weights = 'data/train_valid_res_top_100_dim_35_weights.csv'\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_valid_path = 'data_split/merged_train_valid_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 35\n",
    "\n",
    "# top_n\n",
    "top_n_size = 100\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_valid_path, n_comp)\n",
    "\n",
    "# Загружаем уже посчитанные результаты\n",
    "Recommender.set_similarity_df(path_to_res_users, path_to_res_weights)\n",
    "\n",
    "Recommender.fit(loaded_df=True, top_n_size=top_n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:201: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84416655670f4beb9754b8ed230de981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: [32792,\n",
       "  47766,\n",
       "  24852,\n",
       "  47209,\n",
       "  12000,\n",
       "  19051,\n",
       "  32139,\n",
       "  19156,\n",
       "  1559,\n",
       "  18523,\n",
       "  2002,\n",
       "  46979,\n",
       "  4517,\n",
       "  22474,\n",
       "  16589,\n",
       "  45066,\n",
       "  34688,\n",
       "  22124,\n",
       "  32052,\n",
       "  33754,\n",
       "  17872,\n",
       "  28634,\n",
       "  20039,\n",
       "  32643,\n",
       "  41787,\n",
       "  20574,\n",
       "  48110,\n",
       "  27344,\n",
       "  27966,\n",
       "  7781,\n",
       "  36735,\n",
       "  33276,\n",
       "  45613,\n",
       "  9681,\n",
       "  21376,\n",
       "  46886,\n",
       "  40198,\n",
       "  17758,\n",
       "  20114,\n",
       "  33368,\n",
       "  19348,\n",
       "  2481,\n",
       "  26497,\n",
       "  43509,\n",
       "  41602,\n",
       "  20919,\n",
       "  2240,\n",
       "  43889,\n",
       "  16826,\n",
       "  5713,\n",
       "  13176,\n",
       "  35917,\n",
       "  30489,\n",
       "  16797,\n",
       "  47526,\n",
       "  8479,\n",
       "  8138,\n",
       "  28874,\n",
       "  49451,\n",
       "  37646,\n",
       "  22829,\n",
       "  21150,\n",
       "  47144,\n",
       "  5322,\n",
       "  17224,\n",
       "  38656,\n",
       "  48210,\n",
       "  5907,\n",
       "  14553,\n",
       "  47553,\n",
       "  46676,\n",
       "  24954,\n",
       "  4957,\n",
       "  40571,\n",
       "  28918,\n",
       "  22963,\n",
       "  23,\n",
       "  20084,\n",
       "  5212,\n",
       "  14306,\n",
       "  13742,\n",
       "  18961,\n",
       "  15841,\n",
       "  13351,\n",
       "  5450,\n",
       "  48099,\n",
       "  49273,\n",
       "  47792,\n",
       "  9124,\n",
       "  22559,\n",
       "  33957,\n",
       "  27737,\n",
       "  2573,\n",
       "  4071,\n",
       "  8296,\n",
       "  21227,\n",
       "  25146,\n",
       "  39275,\n",
       "  13083,\n",
       "  49481,\n",
       "  37725,\n",
       "  48171,\n",
       "  10921,\n",
       "  44234,\n",
       "  23734,\n",
       "  44156,\n",
       "  33548,\n",
       "  36144,\n",
       "  39891,\n",
       "  37131,\n",
       "  47145,\n",
       "  8590,\n",
       "  7628,\n",
       "  17264,\n",
       "  8536,\n",
       "  38563,\n",
       "  17341,\n",
       "  30162,\n",
       "  46175,\n",
       "  30338,\n",
       "  429,\n",
       "  4347,\n",
       "  43371,\n",
       "  3512,\n",
       "  7419,\n",
       "  4932,\n",
       "  22000,\n",
       "  40338,\n",
       "  33351,\n",
       "  19850,\n",
       "  28407,\n",
       "  46347,\n",
       "  34092,\n",
       "  32784,\n",
       "  20944,\n",
       "  44771,\n",
       "  24707,\n",
       "  13803,\n",
       "  11609,\n",
       "  2639,\n",
       "  22105,\n",
       "  10521,\n",
       "  24203,\n",
       "  36403,\n",
       "  20147,\n",
       "  852,\n",
       "  32395,\n",
       "  1972,\n",
       "  14061,\n",
       "  47923,\n",
       "  10197,\n",
       "  16178,\n",
       "  4432,\n",
       "  6261,\n",
       "  6136]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посчитаем для 15 юзеров и учтем только топ-3 схожих (3 наиболее похожих юзера для каждого)\n",
    "Recommender.predict(user_id=2, sim_users_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:267: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdaf0df47934824a0e44a008a2b3cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# посчитаем для 15 юзеров и учтем только топ-3 схожих (3 наиболее похожих юзера для каждого)\n",
    "prediction_for_interesting = Recommender.predict_by_count(check_users=user_list, sim_users_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/user_based_interesting_users.pickle', 'wb') as f:\n",
    "    pickle.dump(prediction_for_interesting, f)\n",
    "\n",
    "# with open('data/user_based_first_2k.pickle', 'rb') as f:\n",
    "#     data_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрички"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассматривать большое число соседей не имеет смысла, так как результат либо тот же либо сильно хуже\n",
    "# 5-15 довольно оптимальные числа и по времени занимает меньше, чем предсказывать покупки по 100 схожим юзерам\n",
    "\n",
    "# Размерность декомпозиции тоже влияет, с dim=50 результаты в целом получше, чем с 10, \n",
    "# но считалось это в разы дольше - может имеет смысл взять около 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим данные для проверки метрик\n",
    "Recommender.prepare_validation_data('data_split/validation_1_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:236: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4b3cfcd14b45daac5183915ba63c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# получаем данные предсказанное/реально купленное\n",
    "predicted, actual = Recommender.get_data_to_check_metrics(users_count=100, sim_users_count=50, products_top_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP@k\n",
    "def apk(actual, predicted, k=10):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# MAP@K\n",
    "def mapk(actual, predicted, k=10):\n",
    "    # return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "    return [apk(a,p,k) for a,p in zip(actual, predicted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19666666666666666,\n",
       " 0.15,\n",
       " 0.24952380952380948,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.17666666666666667,\n",
       " 0.2375,\n",
       " 0.13333333333333333,\n",
       " 0.05,\n",
       " 0.05,\n",
       " 0.21666666666666665,\n",
       " 0.2,\n",
       " 0.065,\n",
       " 0.20396825396825397,\n",
       " 0.2544444444444444,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.7761904761904761,\n",
       " 0.2095238095238095,\n",
       " 0.06666666666666667,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.12333333333333334,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.05,\n",
       " 0.2095238095238095,\n",
       " 0.12857142857142856,\n",
       " 0.0,\n",
       " 0.32666666666666666,\n",
       " 0.1,\n",
       " 0.3,\n",
       " 0.6,\n",
       " 0.5,\n",
       " 0.16666666666666666,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.23285714285714287,\n",
       " 0.1,\n",
       " 0.14285714285714285,\n",
       " 0.1,\n",
       " 0.2222222222222222,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.6366666666666667,\n",
       " 0.1,\n",
       " 0.4514285714285714,\n",
       " 0.0,\n",
       " 0.275,\n",
       " 0.01111111111111111,\n",
       " 0.22666666666666666,\n",
       " 0.22666666666666666,\n",
       " 0.0,\n",
       " 0.16666666666666666,\n",
       " 0.05,\n",
       " 0.016666666666666666,\n",
       " 0.26666666666666666,\n",
       " 0.569047619047619,\n",
       " 0.5,\n",
       " 0.315,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.125,\n",
       " 0.1,\n",
       " 0.1,\n",
       " 0.2095238095238095,\n",
       " 0.15,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.2095238095238095,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.1,\n",
       " 0.22999999999999998,\n",
       " 0.15,\n",
       " 0.16666666666666666,\n",
       " 0.5380952380952382,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.513095238095238,\n",
       " 0.12857142857142856,\n",
       " 0.0,\n",
       " 0.31476190476190474,\n",
       " 0.23285714285714282,\n",
       " 0.325,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.275,\n",
       " 0.13999999999999999,\n",
       " 0.0,\n",
       " 0.11666666666666665,\n",
       " 0.1,\n",
       " 0.23958333333333331]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(predicted.values(), actual.values(), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель, которая берет топ-10 продуктов у n схожих пользователей (выбрана как итоговая)\n",
    "# k - число продуктов, которые берем из всех предсказанных (k самых вероятных)\n",
    "\n",
    "# результаты для модели с decomp_dim=35 и было посчитано всего топ-30 схожих пользователей\n",
    "\n",
    "#_________________________________________\n",
    "# среднее по 10 пользователям\n",
    "\n",
    "# если учитывались 500/500 схожих результат был \n",
    "# для k=5 - 0.188\n",
    "# для k=10 - 0.124\n",
    "# для k=30 - 0.067\n",
    "# full - 0.082\n",
    "\n",
    "# если учитывались 200/500 схожих результат был \n",
    "# для k=5 - 0.185\n",
    "# для k=10 - 0.120\n",
    "# для k=30 - 0.065\n",
    "# full - 0.079\n",
    "\n",
    "# если учитывались 30/30 схожих результат был \n",
    "# для k=5 - 0.176\n",
    "# для k=10 - 0.114\n",
    "# для k=30 - 0.066\n",
    "\n",
    "# если учитывались 10/30 схожих результат был \n",
    "# для k=5 - 0.187\n",
    "# для k=10 - 0.117\n",
    "# для k=30 - 0.056\n",
    "\n",
    "# если учитывались 5/30 схожих результат был \n",
    "# для k=5 - 0.15\n",
    "# для k=10 - 0.09\n",
    "# для k=30 - 0.05\n",
    "\n",
    "#_________________________________________\n",
    "# среднее по 100 пользователей, когда не учитывался сам юзер, но менялся размер топа продуктов у схожих\n",
    "\n",
    "# топ-5 продуктов у схожих\n",
    "# если учитывались 50/500 схожих результат был \n",
    "# для k=1 - 0.56\n",
    "# для k=5 - 0.264\n",
    "# для k=10 - 0.156\n",
    "# для k=30 - 0.078\n",
    "# full - 0.065\n",
    "\n",
    "# топ-10 продуктов у схожих\n",
    "# если учитывались 50/500 схожих результат был \n",
    "# для k=1 - 0.54\n",
    "# для k=5 - 0.263\n",
    "# для k=10 - 0.163\n",
    "# для k=30 - 0.082\n",
    "# full - 0.070\n",
    "\n",
    "# топ-15 продуктов у схожих\n",
    "# если учитывались 50/500 схожих результат был \n",
    "# для k=1 - 0.52\n",
    "# для k=5 - 0.259\n",
    "# для k=10 - 0.161\n",
    "# для k=30 - 0.084\n",
    "# full - 0.073\n",
    "\n",
    "\n",
    "#_________________________________________\n",
    "# среднее по 1000 пользователей, когда не учитывался сам юзер\n",
    "\n",
    "# если учитывались 200/500 схожих результат был \n",
    "# для k=5 - 0.240\n",
    "# для k=10 - 0.157\n",
    "# для k=30 - 0.09\n",
    "# full - 0.077\n",
    "\n",
    "# если учитывались 150/500 схожих результат был \n",
    "# для k=5 - 0.241\n",
    "# для k=10 - 0.156\n",
    "# для k=30 - 0.089\n",
    "# full - 0.075\n",
    "\n",
    "# если учитывались 50/500 схожих результат был \n",
    "# для k=1 - 0.51\n",
    "# для k=5 - 0.246\n",
    "# для k=10 - 0.156\n",
    "# для k=30 - 0.087\n",
    "# full - 0.0717\n",
    "\n",
    "# если учитывались 30/500 схожих результат был \n",
    "# для k=5 - 0.242\n",
    "# для k=10 - 0.151\n",
    "# для k=30 - 0.083\n",
    "# full - 0.067\n",
    "\n",
    "# если учитывались 5/500 схожих результат был \n",
    "# для k=5 - 0.210\n",
    "# для k=10 - 0.12\n",
    "# для k=30 - 0.068\n",
    "# full - 0.054\n",
    "\n",
    "#_________________________________________\n",
    "# среднее по 1000 пользователей, когда учитывался сам юзер\n",
    "\n",
    "# если учитывались 50/500 схожих результат был \n",
    "# для k=5 - 0.2549\n",
    "# для k=10 - 0.167\n",
    "# для k=30 - 0.100\n",
    "# full - 0.089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.524"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(predicted.values(), actual.values(), k=1)\n",
    "\n",
    "# модель, которая учитывала все продукты по таблице с 0/1\n",
    "# k - число продуктов, которые берем из всех предсказанных (k самых вероятных)\n",
    "\n",
    "# результаты для модели с decomp_dim=15 и было посчитано всего топ-30 схожих пользователей\n",
    "\n",
    "#_________________________________________\n",
    "# среднее по 100 пользователям\n",
    "\n",
    "# если учитывались 30/30 схожих результат был \n",
    "# для k=5 - 0.23\n",
    "# для k=10 - 0.15\n",
    "# для k=30 - 0.082\n",
    "\n",
    "# если учитывались 10/30 схожих результат был \n",
    "# для k=5 - 0.24\n",
    "# для k=10 - 0.16\n",
    "# для k=30 - 0.098\n",
    "\n",
    "# если учитывались 5/30 схожих результат был \n",
    "# для k=5 - 0.26\n",
    "# для k=10 - 0.18\n",
    "# для k=30 - 0.12\n",
    "\n",
    "# то есть чем больше схожих юзеров учитываем, тем меньше точность предсказания\n",
    "\n",
    "\n",
    "#_________________________________________\n",
    "# рассмотрим среднее по 1000 пользователей\n",
    "\n",
    "# если учитывались 3/30 схожих результат был \n",
    "# для k=1 - 0.524\n",
    "# для k=5 - 0.317\n",
    "# для k=10 - 0.246\n",
    "# для k=30 - 0.17\n",
    "\n",
    "# если учитывались 5/30 схожих результат был \n",
    "# для k=1 - 0.507\n",
    "# для k=5 - 0.26\n",
    "# для k=10 - 0.20\n",
    "# для k=30 - 0.14\n",
    "\n",
    "# если учитывались 15/30 схожих результат был \n",
    "# для k=1 - 0.498\n",
    "# для k=5 - 0.229\n",
    "# для k=10 - 0.15\n",
    "# для k=30 - 0.097\n",
    "\n",
    "# неплохое значение для первого предсказанного продукта (самый вероятный, который выдала модель)\n",
    "\n",
    "# возможно увеличение размерности декомпозиции изменит тенденцию увеличения точности при уменьшении кол-ва учитываемых соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1726690058893928"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(predicted.values(), actual.values(), k=10)\n",
    "\n",
    "# модель, которая учитывала все продукты по таблице с 0/1\n",
    "# k - число продуктов, которые берем из всех предсказанных (k самых вероятных)\n",
    "\n",
    "# результаты для модели с decomp_dim=20 и было посчитано всего топ-30 схожих пользователей\n",
    "\n",
    "#_________________________________________\n",
    "# среднее по 100 пользователям\n",
    "\n",
    "# если учитывались 30/30 схожих результат был \n",
    "# для k=1 - 0.56\n",
    "# для k=5 - 0.239\n",
    "# для k=10 - 0.151\n",
    "# для k=30 - 0.08\n",
    "\n",
    "# если учитывались 10/30 схожих результат был \n",
    "# для k=1 - 0.56\n",
    "# для k=5 - 0.249\n",
    "# для k=10 - 0.159\n",
    "# для k=30 - 0.95\n",
    "\n",
    "# если учитывались 5/30 схожих результат был \n",
    "# для k=1 - 0.61\n",
    "# для k=5 - 0.287\n",
    "# для k=10 - 0.19\n",
    "# для k=30 - 0.13\n",
    "\n",
    "# то есть чем больше схожих юзеров учитываем, тем ниже точность предсказания\n",
    "\n",
    "\n",
    "#_________________________________________\n",
    "# рассмотрим среднее по 1000 пользователей\n",
    "\n",
    "# если учитывались 1/30 схожих результат был \n",
    "# для k=1 - 0.661\n",
    "# для k=5 - 0.480\n",
    "# для k=10 - 0.391\n",
    "# для k=30 - 0.279\n",
    "# full - 0.216\n",
    "\n",
    "# если учитывались 3/30 схожих результат был \n",
    "# для k=1 - 0.56\n",
    "# для k=5 - 0.329\n",
    "# для k=10 - 0.257\n",
    "# для k=30 - 0.185\n",
    "# full - 0.153\n",
    "\n",
    "# если учитывались 5/30 схожих результат был \n",
    "# для k=1 - 0.518\n",
    "# для k=5 - 0.284\n",
    "# для k=10 - 0.213\n",
    "# для k=30 - 0.150\n",
    "# full - 0.128\n",
    "\n",
    "# если учитывались 10/30 схожих результат был \n",
    "# для k=1 - 0.508\n",
    "# для k=5 - 0.245\n",
    "# для k=10 - 0.172\n",
    "# для k=30 - 0.113\n",
    "# full - 0.103\n",
    "\n",
    "# если учитывались 15/30 схожих результат был \n",
    "# для k=1 - 0.504\n",
    "# для k=5 - 0.24\n",
    "# для k=10 - 0.162\n",
    "# для k=30 - 0.101\n",
    "# full - 0.0937\n",
    "\n",
    "# если учитывались 30/30 схожих результат был \n",
    "# для k=1 - 0.505\n",
    "# для k=5 - 0.233\n",
    "# для k=10 - 0.152\n",
    "# для k=30 - 0.092\n",
    "# full - 0.084\n",
    "\n",
    "\n",
    "# неплохое значение для первого предсказанного продукта (самый вероятный, который выдала модель)\n",
    "\n",
    "# возможно увеличение размерности декомпозиции изменит тенденцию увеличения точности при уменьшении кол-ва учитываемых соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk(predicted.values(), actual.values(), k=10)\n",
    "\n",
    "# модель, которая учитывала все продукты по таблице с 0/1\n",
    "# k - число продуктов, которые берем из всех предсказанных (k самых вероятных)\n",
    "\n",
    "# результаты для модели с decomp_dim=35 и было посчитано всего топ-30 схожих пользователей\n",
    "\n",
    "\n",
    "# чем больше схожих юзеров учитываем, тем ниже точность предсказания\n",
    "\n",
    "\n",
    "#_________________________________________\n",
    "# рассмотрим среднее по 1000 пользователей\n",
    "\n",
    "\n",
    "# если учитывались 1/30 схожих результат был \n",
    "# для k=1 - 0.662\n",
    "# для k=5 - 0.4812\n",
    "# для k=10 - 0.3921\n",
    "# для =15 - 0.3450\n",
    "# для k=30 - 0.2803\n",
    "# full - 0.2174\n",
    "\n",
    "# если учитывались 3/30 схожих результат был \n",
    "# для k=1 - 0.556\n",
    "# для k=5 - 0.3420\n",
    "# для k=10 - 0.2694\n",
    "# для k=15 - 0.2345\n",
    "# для k=30 - 0.1911\n",
    "# full - 0.1568\n",
    "\n",
    "# если учитывались 5/30 схожих результат был \n",
    "# для k=1 - 0.54\n",
    "# для k=5 - 0.2955\n",
    "# для k=10 - 0.2230\n",
    "# для k=15 - 0.1911\n",
    "# для k=30 - 0.1558\n",
    "# full - 0.1328\n",
    "\n",
    "# если учитывались 10/30 схожих результат был \n",
    "# для k=1 - 0.528\n",
    "# для k=5 - 0.2621\n",
    "# для k=10 - 0.1862\n",
    "# для k=15 - 0.1536\n",
    "# для k=30 - 0.1197\n",
    "# full - 0.1068\n",
    "\n",
    "# если учитывались 15/30 схожих результат был \n",
    "# для k=1 - 0.524\n",
    "# для k=5 - 0.2565\n",
    "# для k=10 - 0.1742\n",
    "# для k=15 - 0.1411\n",
    "# для k=30 - 0.1079\n",
    "# full - 0.0977\n",
    "\n",
    "# если учитывались 30/30 схожих результат был \n",
    "# для k=1 - 0.515\n",
    "# для k=5 - 0.251\n",
    "# для k=10 - 0.163\n",
    "# для k=30 - 0.092\n",
    "\n",
    "\n",
    "# неплохое значение для первого предсказанного продукта (самый вероятный, который выдала модель)\n",
    "\n",
    "# возможно увеличение размерности декомпозиции изменит тенденцию увеличения точности при уменьшении кол-ва учитываемых соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорены методы fit(), get_similarity(), predict(), get_data_to_check_metrics()\n",
    "\n",
    "fit():\n",
    "   + вынесена self.decompose() для выполнения только в том случае, если нужно полное обучение модели\n",
    "   \n",
    "get_similarity():\n",
    "   + ранее считал для размерности декомпозиции 10 около 3 часов, сейчас это занимает порядка полутора часов (ускорилось благодаря счету не каждого юзера отдельно, а по 100 за раз, те матричные операции выполняются быстрее)\n",
    "   + теперь считает как номера пользователей, так и их веса (те теперь считается и сохраняется 2 датафрейма, как рез-т обучения)\n",
    "   + использован другой способ сортировки данных (полагаю это тоже ускорило процесс)\n",
    "   \n",
    "predict():\n",
    "   + Из цикла вынесена операция from_coo, занимавшая порядка секунды, вытащить из уже посчитанной структуры необходимые данные по индексу уже сущие копейки\n",
    "   + Не считает заново веса, так как они теперь считаются в get_similarity()\n",
    "   + Исходя из параметров может посчитать для одного юзера с опеределенным id/для всех юзеров/для части юзеров, начиная с 1; также есть параметр, указывающий сколько из топа схожих юзеров надо учесть\n",
    "   + По итогу на весь датасет, на который ранее требовалось 4500 часов, сейчас требуется порядка 7 часов, если уменьшить число пользователей, по которым сделаем итоговую оценку, то управится со всеми данными примерно за полтора часа для 5 и за 2.5 часа для 10.\n",
    "   \n",
    "get_data_to_check_metrics():\n",
    "   + Ускорен засчет оптимизации вышеописанных методов\n",
    "   \n",
    "Можем рассчитать любой топ пользователей, но метрика оказывается больше, если учитываем меньшее число схожих юзеров в predict(). Численные рез-ты полученные при подсчете метрик и некоторые комментарии в п. Метрички (см. комментарии предыдущей ячейки)\n",
    "\n",
    "\n",
    "\n",
    "#### Для user-based модели выбраны гиперпараметры:\n",
    "   + Число схожих юзеров - 50\n",
    "   + Размер декомпозиции - 35\n",
    "   + Размер топа продуктов схожих пользователей - 10\n",
    "   \n",
    "Изменения в модели: ранее учитывала только факт покупки и брала все предметы, купленные схожими юзерами из топа, теперь рассматривает только k популярных продуктов (исходя из кол-ва купленного) у схожих юзеров из топа и НЕ учитывает самого юзера для которого делается предсказание (формулировка по факту - рекомендуем то, что покупает чаще всего похожие на вас пользователи)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
