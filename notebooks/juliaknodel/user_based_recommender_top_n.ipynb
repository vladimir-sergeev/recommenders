{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Класс реализованного метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedRecommender():\n",
    "    def __init__(self, path_to_orders, path_to_order_products_train, decomposition_dim):\n",
    "        '''\n",
    "        :param path_to_orders: путь к исходному датафрейму с keggle orders.csv\n",
    "        :param path_to_order_products_train: путь к новому тренировочному датафрейму, полученному в п.II ниже\n",
    "        :param decomposition_dim: размерность декомпозиции\n",
    "        '''\n",
    "        self.path_to_orders = path_to_orders\n",
    "        self.path_to_order_products_train = path_to_order_products_train\n",
    "        self.n_comp = decomposition_dim\n",
    "        \n",
    "        self.ok_status = True\n",
    "        check, message = self.check_input_params()\n",
    "        \n",
    "        if not check:\n",
    "            self.ok_status = False\n",
    "            print(message)\n",
    "    \n",
    "    def check_input_params(self):\n",
    "        if not os.path.isfile(self.path_to_orders):\n",
    "            return False, 'File doesnt exist'\n",
    "        if not os.path.isfile(self.path_to_order_products_train):\n",
    "            return False, 'File doesnt exist'\n",
    "        if not isinstance(self.n_comp, int) or not self.n_comp > 0:\n",
    "            return False, 'decomposition_dim should be positive integer value'\n",
    "        \n",
    "        return True, 'Ok'\n",
    "            \n",
    "    def read_data(self):\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.orders = pd.read_csv(self.path_to_orders)\n",
    "        self.order_products_train = pd.read_csv(self.path_to_order_products_train)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        '''\n",
    "        создает таблицу user-item для дальнейших манипуляций\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.merged = pd.merge(self.orders, self.order_products_train, on='order_id', how='right')\n",
    "        self.merged = self.merged[['order_id', 'user_id', 'product_id']]\n",
    "        \n",
    "        self.user_id_num = {key: value for value, key in enumerate(self.merged['user_id'].unique(), start=0)}\n",
    "        self.product_id_num = {key: value for value, key in enumerate(self.merged['product_id'].unique(), start=0)}\n",
    "        self.product_num_id = {value: key for value, key in enumerate(self.merged['product_id'].unique(), start=0)}\n",
    "        \n",
    "        self.merged['user_num'] = self.merged['user_id'].transform(lambda x: self.user_id_num[x])\n",
    "        self.merged['product_num'] = self.merged['product_id'].transform(lambda x: self.product_id_num[x])\n",
    "        self.merged['buy'] = 1\n",
    "        \n",
    "        self.user_item_matrix = sps.coo_matrix((self.merged.buy, (self.merged.user_num, self.merged.product_num)))\n",
    "        \n",
    "    def decompose(self):\n",
    "        '''\n",
    "        декомпозиция user-item_matrix с размерностью n_comp\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.model = NMF(n_components=self.n_comp, init='random', random_state=0)\n",
    "        self.user_matrix = self.model.fit_transform(self.user_item_matrix)\n",
    "        self.product_matrix = self.model.components_\n",
    "    \n",
    "    def get_similarity(self, top_n_size):\n",
    "        '''Каждому юзеру сопоставляет id n-схожих юзеров\n",
    "        :param top_n_size: сколько схожих пользователей по убыванию схожести надо сохранить\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        \n",
    "        self.similar_users_df = pd.DataFrame()\n",
    "        self.similar_users_weight_df = pd.DataFrame()\n",
    "            \n",
    "        size = 100\n",
    "        users_cnt = len(self.user_matrix)\n",
    "        add = 1 if users_cnt % size > 0 else 0\n",
    "        parts_cnt = users_cnt//size + add\n",
    "\n",
    "        for i in tqdm(range(parts_cnt)):\n",
    "            start = size * i\n",
    "            end = size * i + size\n",
    "                \n",
    "            user_to_others_similarity = cosine_similarity(Y=self.user_matrix, X=self.user_matrix[start:end])\n",
    "                \n",
    "            sim_users_df = pd.DataFrame(np.argsort(-user_to_others_similarity)).loc[:, :top_n_size]\n",
    "            sim_users_weight_df = pd.DataFrame(-np.sort(-user_to_others_similarity)).loc[:, :top_n_size]\n",
    "                \n",
    "            self.similar_users_df = pd.concat([self.similar_users_df, sim_users_df], ignore_index=True)\n",
    "            self.similar_users_weight_df = pd.concat([self.similar_users_weight_df, sim_users_weight_df], ignore_index=True)\n",
    "    \n",
    "    def save_similarity_df(self, path_users, path_weights):\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.similar_users_df.to_csv(path_users, index=False)\n",
    "        self.similar_users_weight_df.to_csv(path_weights, index=False)\n",
    "        \n",
    "    def set_similarity_df(self, path_to_df_users, path_to_df_weights):\n",
    "        '''\n",
    "        Если уже есть посчитанная матрица, то можем использовать и обойти долгий кусок обучения в fit(),\n",
    "        указав параметр loaded_df=True\n",
    "        :param path_to_df: путь до csv файла с учебным датафреймом\n",
    "        '''\n",
    "        if os.path.isfile(path_to_df_users) and os.path.isfile(path_to_df_weights):\n",
    "            self.similar_users_df = pd.read_csv(path_to_df_users)\n",
    "            self.similar_users_weight_df = pd.read_csv(path_to_df_weights)\n",
    "        else:\n",
    "            # оформить ошибки по-человечески - нужен же возврат к True\n",
    "            # self.ok_status = False\n",
    "            return 'File doesnt exist'\n",
    "    \n",
    "    def fit(self, path_to_save_users=False, path_to_save_weights=False, loaded_df=False, top_n_size=5):\n",
    "        '''\n",
    "        :param self:\n",
    "        :param path_to_save: путь для сохранения матрицы сх. юзеров и весов (названия дефолтные)\n",
    "        :param loaded_df: bool - True если матрица уже была загружена и пересчитывать заново ее не нужно;\n",
    "                          default=False\n",
    "        :param top_n_size: размер топ схожих пользователей \n",
    "        :return: информация в случае ошибки \n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.top_n_size = top_n_size\n",
    "        \n",
    "        self.read_data()     # читает необходимые df\n",
    "        self.prepare_data()  # создание таблицы user-item с необх. полями и разметками\n",
    "        if not loaded_df:\n",
    "            self.decompose()     # декомпозиция матрицы на 2 матрицы размерности n\n",
    "            self.get_similarity(self.top_n_size)\n",
    "            if path_to_save_users and path_to_save_weights:\n",
    "                self.save_similarity_df(path_to_save_users, path_to_save_weights)\n",
    "    \n",
    "    def predict(self, user_id=False, sim_users_count=False, users_count=False):\n",
    "        '''\n",
    "        :param self:\n",
    "        :param user_id:  id юзера, для которого считаем предсказание покупок, если False, то считаем всех\n",
    "        :param sim_users_count: если нужно учесть не весь топ схожих юзеров, а только его часть, \n",
    "                                равную значению данного параметра\n",
    "        :param users_count: если нужно посчитать не для одного/всех юзеров, а для части, \n",
    "                                равной значению данного параметра\n",
    "        :return: сообщение в случае ошибки; в случае успеха - словарь {user_id: products_id}\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        if not sim_users_count or sim_users_count > self.top_n_size:\n",
    "            sim_users_count = self.top_n_size\n",
    "        \n",
    "        if user_id:\n",
    "            if user_id in self.user_id_num:\n",
    "                users_to_check = [user_id] \n",
    "            else:\n",
    "                return 'There is no user with this id'\n",
    "        else:\n",
    "            users_to_check = [*self.user_id_num][:users_count] if users_count else [*self.user_id_num]\n",
    "            \n",
    "        self.predicted_products = dict()\n",
    "            \n",
    "        users_purchases = pd.Series.sparse.from_coo(self.user_item_matrix)\n",
    "        \n",
    "        # проход по всем юзерам, для которых хотим получить предсказание покупок\n",
    "        for user_id in tqdm(users_to_check):\n",
    "            user_num = self.get_user_num(user_id)\n",
    "            \n",
    "            # номера и веса схожих c user_id пользователей\n",
    "            sim_users_nums = self.similar_users_df.loc[user_num].values\n",
    "            sim_users_weights = self.similar_users_weight_df.loc[user_num].values\n",
    "\n",
    "            d = defaultdict(int)\n",
    "            \n",
    "            # проход по всем юзерам схожим с предсказываемым\n",
    "            for sim_user_num, sim_user_weight in zip(sim_users_nums[:sim_users_count], sim_users_weights[:sim_users_count]):\n",
    "\n",
    "                sim_user_purchases = users_purchases[sim_user_num].index\n",
    "                \n",
    "                # к каждому купленному схожим юзером продукту прибавляем его вес относительно предсказываемого\n",
    "                for product in sim_user_purchases:\n",
    "                        d[product] += sim_user_weight\n",
    "\n",
    "            d_ids = defaultdict(int)\n",
    "\n",
    "            # ключами были номера продуктов - заменим на id\n",
    "            for key in d:\n",
    "                d_ids[self.product_num_id[key]] = d[key]\n",
    "                \n",
    "            self.predicted_products[user_id] = [*dict(sorted(d_ids.items(), key=lambda x: x[1], reverse=True))]\n",
    "        return self.predicted_products\n",
    "    \n",
    "    def get_coo(self):\n",
    "        return self.user_item_matrix\n",
    "    \n",
    "    def get_user_num(self, user_id):\n",
    "        return self.user_id_num[user_id]\n",
    "    \n",
    "    def prepare_validation_data(self, path_to_valid_df):\n",
    "        '''\n",
    "        создает таблицу user-item для дальнейших манипуляций\n",
    "        :param path_to_valid_df: путь до csv таблицы с валидационным датафреймом\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.order_products_valid = pd.read_csv(path_to_valid_df)\n",
    "        \n",
    "        self.merged_valid = pd.merge(self.orders, self.order_products_valid, on='order_id', how='right')\n",
    "        self.merged_valid = self.merged_valid[['order_id', 'user_id', 'product_id']]\n",
    "        \n",
    "        self.user_id_num_valid = {key: value for value, key in enumerate(self.merged_valid['user_id'].unique(), start=0)}\n",
    "        self.product_id_num_valid = {key: value for value, key in enumerate(self.merged_valid['product_id'].unique(), start=0)}\n",
    "        self.product_num_id_valid = {value: key for value, key in enumerate(self.merged_valid['product_id'].unique(), start=0)}\n",
    "        \n",
    "        self.merged_valid['user_num'] = self.merged_valid['user_id'].transform(lambda x: self.user_id_num_valid[x])\n",
    "        self.merged_valid['product_num'] = self.merged_valid['product_id'].transform(lambda x: self.product_id_num_valid[x])\n",
    "        self.merged_valid['buy'] = 1\n",
    "        \n",
    "        self.user_item_matrix_valid = sps.coo_matrix((self.merged_valid.buy, (self.merged_valid.user_num, self.merged_valid.product_num)))\n",
    "        \n",
    "    def get_data_to_check_metrics(self, users_count=False, sim_users_count=False):\n",
    "        '''\n",
    "        :param count_users: для скольких юзеров посчитаем метрики\n",
    "        :param sim_users_count: сколько юзеров из топа схожих будут учитываться для предсказания покупок\n",
    "        '''\n",
    "        predicted_purchases = {}\n",
    "        actual_purchases = {}\n",
    "        \n",
    "        predicted = self.predict(sim_users_count=sim_users_count, users_count=users_count)\n",
    "        \n",
    "        users_purchases_valid = pd.Series.sparse.from_coo(self.user_item_matrix_valid)\n",
    "        \n",
    "        for user_id in predicted:\n",
    "            if user_id in self.user_id_num_valid:\n",
    "                user_num_in_valid_df = self.user_id_num_valid[user_id]\n",
    "                user_actual_purchases_nums = users_purchases_valid[user_num_in_valid_df].index\n",
    "                    \n",
    "                actual = [self.product_num_id_valid[product_num] for product_num in user_actual_purchases_nums]\n",
    "            \n",
    "                predicted_purchases[user_id] = predicted[user_id]\n",
    "                actual_purchases[user_id] = actual\n",
    "            \n",
    "            else:\n",
    "                print('No user with id: ', user_id)\n",
    "        \n",
    "        return actual_purchases, predicted_purchases\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Пропишем все пути и сконструируем датасеты мечты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# необходимые файлы с исходными датасетами с keggle\n",
    "path_data_train = 'D:/Projects/Anaconda/EDA/data/order_products__train.csv'\n",
    "path_data_prior = 'D:/Projects/Anaconda/EDA/data/order_products__prior.csv'\n",
    "path_data_orders = 'D:/Projects/Anaconda/EDA/data/orders.csv'\n",
    "    \n",
    "# [куда запишем]/[где уже лежат] файлы с индексами\n",
    "train_path = 'data_split/train_indices.pickle'\n",
    "valid_path = 'data_split/validation_indices.pickle'\n",
    "test_path = 'data_split/test_indices.pickle'\n",
    "\n",
    "# куда запишем файлы с новым разделением\n",
    "df_train_path = 'data_split/train_data.csv'\n",
    "df_valid_path = 'data_split/validation_data.csv'\n",
    "df_test_path = 'data_split/test_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Если файлов с индексами нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_indices(grouped_ratings, retriever):\n",
    "    return np.concatenate(grouped_ratings.apply(retriever).values)\n",
    "\n",
    "def split(path_to_orders):\n",
    "    orders = pd.read_csv(path_to_orders)\n",
    "    grouper = orders.sort_values('order_number').groupby('user_id')\n",
    "    train_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings[:int(user_ratings.shape[0] * 0.5)].index.values)\n",
    "    \n",
    "    validation_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings.iloc[int(user_ratings.shape[0] * 0.5):\n",
    "                                               int(user_ratings.shape[0] * 0.75)].index.values)\n",
    "    \n",
    "    test_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings.iloc[int(user_ratings.shape[0] * 0.75):].index.values)\n",
    "    \n",
    "    return train_indices, validation_indices, test_indices\n",
    "\n",
    "\n",
    "train_indices, validation_indices, test_indices = split(path_data_orders)\n",
    "\n",
    "# save results\n",
    "with open(train_path, 'wb') as out:\n",
    "    pickle.dump(train_indices, out)\n",
    "\n",
    "with open(valid_path, 'wb') as out:\n",
    "    pickle.dump(validation_indices, out)\n",
    "\n",
    "with open(test_path, 'wb') as out:\n",
    "    pickle.dump(test_indices, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файлы с индексами готовы - теперь скомпонуем и запишем в файл все необходимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concat_same_data(df_train_path, df_prior_path, df_orders_path):\n",
    "    \n",
    "    df_train = pd.read_csv(df_train_path)\n",
    "    df_prior = pd.read_csv(df_prior_path)\n",
    "    df_orders = pd.read_csv(df_orders_path)\n",
    "    \n",
    "    order_products_full = pd.concat([df_train, df_prior])\n",
    "    \n",
    "    return order_products_full, df_orders\n",
    "\n",
    "\n",
    "def get_split_df(train_path, valid_path, test_path, df_orders, df_order_products_full):\n",
    "    \n",
    "    with open(train_path, 'rb') as input:\n",
    "        train_indices = pickle.load(input)\n",
    "\n",
    "    with open(valid_path, 'rb') as input:\n",
    "        validation_indices = pickle.load(input)\n",
    "\n",
    "    with open(test_path, 'rb') as input:\n",
    "        test_indices = pickle.load(input)\n",
    "        \n",
    "    train_order_ids = df_orders.loc[train_indices]['order_id'].values\n",
    "    validation_order_ids = df_orders.loc[validation_indices]['order_id'].values\n",
    "    test_order_ids = df_orders.loc[test_indices]['order_id'].values\n",
    "    \n",
    "    train_df = df_order_products_full.loc[train_order_ids]\n",
    "    validation_df = df_order_products_full.loc[validation_order_ids]\n",
    "    test_df = df_order_products_full.loc[test_order_ids]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "# считается что уже имеем 3 файла с индексами\n",
    "df_order_products_full, df_orders = read_and_concat_same_data(path_data_train, path_data_prior, path_data_orders)\n",
    "train_df, validation_df, test_df = get_split_df(train_path, valid_path, test_path, df_orders, df_order_products_full)\n",
    "\n",
    "# write new df's to file:\n",
    "train_df.to_csv(df_train_path, index=False)\n",
    "validation_df.to_csv(df_valid_path, index=False)\n",
    "test_df.to_csv(df_test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Посчитаем с новым алгоритмом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# файл, куда по необходимости загрузим результаты\n",
    "path_to_res_users = 'data/res_top_30_dim_15_users.csv'\n",
    "path_to_res_weights = 'data/res_top_30_dim_15_weights.csv'\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_path = 'data_split/train_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 15\n",
    "\n",
    "# top_n\n",
    "top_n_size = 30\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_path, n_comp)\n",
    "\n",
    "Recommender.fit(path_to_save_users=path_to_res_users, path_to_save_weights=path_to_res_weights, top_n_size=top_n_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример без обучения (просто загрузка рез-тов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# файл, куда по необходимости загрузим результаты\n",
    "path_to_res_users = 'data/res_top_30_dim_15_users.csv'\n",
    "path_to_res_weights = 'data/res_top_30_dim_15_weights.csv'\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_path = 'data_split/train_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 15\n",
    "\n",
    "# top_n\n",
    "top_n_size = 30\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_path, n_comp)\n",
    "\n",
    "# Загружаем уже посчитанные результаты\n",
    "Recommender.set_similarity_df(path_to_res_users, path_to_res_weights)\n",
    "\n",
    "Recommender.fit(loaded_df=True, top_n_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:167: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5d66f63414456d91091d246e69e813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: [27845,\n",
       "  23826,\n",
       "  26405,\n",
       "  39657,\n",
       "  46149,\n",
       "  32478,\n",
       "  43154,\n",
       "  3798,\n",
       "  35336,\n",
       "  44683,\n",
       "  2120,\n",
       "  33065,\n",
       "  23504,\n",
       "  30162]}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посчитаем для 5 юзеров и у каждого учтем только топ-3 схожих\n",
    "Recommender.predict(users_count=1, sim_users_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:167: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7900b2d2eb5444dbe2c665b7d101f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{10: [17795,\n",
       "  28535,\n",
       "  9339,\n",
       "  40604,\n",
       "  46979,\n",
       "  16797,\n",
       "  34969,\n",
       "  10749,\n",
       "  25230,\n",
       "  22825,\n",
       "  40706,\n",
       "  13212,\n",
       "  27104,\n",
       "  45664,\n",
       "  30489,\n",
       "  11782,\n",
       "  41220,\n",
       "  16857,\n",
       "  13512,\n",
       "  23165,\n",
       "  32299,\n",
       "  36695,\n",
       "  15011,\n",
       "  43014,\n",
       "  28986,\n",
       "  47380,\n",
       "  47591,\n",
       "  22035,\n",
       "  47526,\n",
       "  23541,\n",
       "  42625,\n",
       "  36865,\n",
       "  9871,\n",
       "  13829,\n",
       "  28842,\n",
       "  31506,\n",
       "  48720,\n",
       "  29650,\n",
       "  21616,\n",
       "  45007,\n",
       "  45066,\n",
       "  17553,\n",
       "  23765,\n",
       "  33731,\n",
       "  8277,\n",
       "  49131,\n",
       "  7781,\n",
       "  37947,\n",
       "  28465,\n",
       "  43122,\n",
       "  43933,\n",
       "  17579,\n",
       "  26521,\n",
       "  46359,\n",
       "  20015,\n",
       "  34423,\n",
       "  2954,\n",
       "  39694,\n",
       "  38300,\n",
       "  42265,\n",
       "  20940,\n",
       "  13424,\n",
       "  8571,\n",
       "  4724,\n",
       "  1957]}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посчитаем для юзера с id 125 и учтем только топ-3 схожих\n",
    "Recommender.predict(sim_users_count=3, user_id=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрички"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассматривать большое число соседей не имеет смысла, так как результат либо тот же либо сильно хуже\n",
    "# 5-15 довольно оптимальные числа и по времени занимает меньше, чем предсказывать покупки по 100 схожим юзерам\n",
    "\n",
    "# Размерность декомпозиции тоже влияет, с dim=50 результаты в целом получше, чем с 10, \n",
    "# но считалось это в разы дольше - может имеет смысл взять около 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим данные для проверки метрик\n",
    "Recommender.prepare_validation_data('data_split/validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:167: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5873e944d5814179904357bf36513843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No user with id:  4\n",
      "No user with id:  13\n",
      "No user with id:  22\n",
      "No user with id:  30\n",
      "No user with id:  66\n",
      "No user with id:  72\n",
      "No user with id:  81\n",
      "No user with id:  92\n",
      "No user with id:  116\n",
      "No user with id:  120\n",
      "No user with id:  139\n",
      "No user with id:  178\n",
      "No user with id:  198\n",
      "No user with id:  201\n",
      "No user with id:  225\n",
      "No user with id:  240\n",
      "No user with id:  242\n",
      "No user with id:  260\n",
      "No user with id:  271\n",
      "No user with id:  275\n",
      "No user with id:  282\n",
      "No user with id:  285\n",
      "No user with id:  314\n",
      "No user with id:  317\n",
      "No user with id:  326\n",
      "No user with id:  344\n",
      "No user with id:  365\n",
      "No user with id:  377\n",
      "No user with id:  383\n",
      "No user with id:  384\n",
      "No user with id:  395\n",
      "No user with id:  400\n",
      "No user with id:  408\n",
      "No user with id:  431\n",
      "No user with id:  436\n",
      "No user with id:  441\n",
      "No user with id:  456\n",
      "No user with id:  461\n",
      "No user with id:  472\n",
      "No user with id:  477\n",
      "No user with id:  483\n",
      "No user with id:  518\n",
      "No user with id:  525\n",
      "No user with id:  549\n",
      "No user with id:  561\n",
      "No user with id:  563\n",
      "No user with id:  598\n",
      "No user with id:  617\n",
      "No user with id:  619\n",
      "No user with id:  622\n",
      "No user with id:  639\n",
      "No user with id:  644\n",
      "No user with id:  654\n",
      "No user with id:  666\n",
      "No user with id:  675\n",
      "No user with id:  677\n",
      "No user with id:  697\n",
      "No user with id:  702\n",
      "No user with id:  715\n",
      "No user with id:  733\n",
      "No user with id:  750\n",
      "No user with id:  759\n",
      "No user with id:  776\n",
      "No user with id:  783\n",
      "No user with id:  788\n",
      "No user with id:  790\n",
      "No user with id:  796\n",
      "No user with id:  807\n",
      "No user with id:  825\n",
      "No user with id:  836\n",
      "No user with id:  871\n",
      "No user with id:  882\n",
      "No user with id:  901\n",
      "No user with id:  909\n",
      "No user with id:  910\n",
      "No user with id:  934\n",
      "No user with id:  948\n",
      "No user with id:  949\n",
      "No user with id:  952\n",
      "No user with id:  971\n",
      "No user with id:  986\n",
      "No user with id:  991\n",
      "No user with id:  1014\n",
      "No user with id:  1026\n",
      "No user with id:  1053\n",
      "No user with id:  1073\n",
      "No user with id:  1081\n",
      "No user with id:  1094\n",
      "No user with id:  1099\n",
      "No user with id:  1103\n",
      "No user with id:  1111\n",
      "No user with id:  1125\n",
      "No user with id:  1127\n",
      "No user with id:  1136\n",
      "No user with id:  1149\n",
      "No user with id:  1153\n",
      "No user with id:  1163\n",
      "No user with id:  1168\n"
     ]
    }
   ],
   "source": [
    "# получаем данные предсказанное/реально купленное\n",
    "predicted, actual = Recommender.get_data_to_check_metrics(users_count=1000, sim_users_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AP@k\n",
    "def apk(actual, predicted, k=10):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# MAP@K\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04320114060241818"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk(predicted.values(), actual.values(), k=10)\n",
    "# среднее по 1000 пользователей\n",
    "# если учитывались 2 схожих результат был 0.048\n",
    "# для 5 схожих 0.043\n",
    "# для 10-20... <0.034\n",
    "\n",
    "# то есть чем больше схожих юзеров учитываем, тем ниже становится точность (хотя она и так не сильно большая)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорены методы fit(), get_similarity(), predict(), get_data_to_check_metrics()\n",
    "\n",
    "fit():\n",
    "   + вынесена self.decompose() для выполнения только в том случае, если нужно полное обучение модели\n",
    "   \n",
    "get_similarity():\n",
    "   + ранее считал для размерности декомпозиции 10 около 3 часов, сейчас это занимает порядка полутора часов (ускорилось благодаря счету не каждого юзера отдельно, а по 100 за раз, те матричные операции выполняются быстрее)\n",
    "   + теперь считает как номера пользователей, так и их веса (те теперь считается и сохраняется 2 датафрейма, как рез-т обучения)\n",
    "   + использован другой способ сортировки данных (полагаю это тоже ускорило процесс)\n",
    "   \n",
    "predict():\n",
    "   + Из цикла вынесена операция from_coo, занимавшая порядка секунды, вытащить из уже посчитанной структуры необходимые данные по индексу уже сущие копейки\n",
    "   + Не считает заново веса, так как они теперь считаются в get_similarity()\n",
    "   + Исходя из параметров может посчитать для одного юзера с опеределенным id/для всех юзеров/для части юзеров, начиная с 1; также есть параметр, указывающий сколько из топа схожих юзеров надо учесть\n",
    "   + По итогу на весь датасет, на который ранее требовалось 4500 часов, сейчас требуется порядка 7 часов, если уменьшить число пользователей, по которым сделаем итоговую оценку, то управится со всеми данными примерно за полтора часа для 5 и за 2.5 часа для 10.\n",
    "   \n",
    "get_data_to_check_metrics():\n",
    "   + Ускорен засчет оптимизации вышеописанных методов\n",
    "   \n",
    "Можем рассчитать любой топ пользователей, но метрика оказывается больше, если учитываем меньшее число схожих юзеров в predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
