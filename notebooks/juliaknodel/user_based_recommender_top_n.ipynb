{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Класс реализованного метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedRecommender():\n",
    "    def __init__(self, path_to_orders, path_to_order_products_train, decomposition_dim):\n",
    "        \n",
    "        self.path_to_orders = path_to_orders\n",
    "        self.path_to_order_products_train = path_to_order_products_train\n",
    "        self.n_comp = decomposition_dim\n",
    "        \n",
    "        self.ok_status = True\n",
    "        check, message = self.check_input_params()\n",
    "        \n",
    "        if not check:\n",
    "            self.ok_status = False\n",
    "            print(message)\n",
    "    \n",
    "    def check_input_params(self):\n",
    "        if not os.path.isfile(self.path_to_orders):\n",
    "            return False, 'File doesnt exist'\n",
    "        if not os.path.isfile(self.path_to_order_products_train):\n",
    "            return False, 'File doesnt exist'\n",
    "        if not isinstance(self.n_comp, int) or not self.n_comp > 0:\n",
    "            return False, 'decomposition_dim should be positive integer value'\n",
    "        \n",
    "        return True, 'Ok'\n",
    "            \n",
    "    def read_data(self):\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.orders = pd.read_csv(self.path_to_orders)\n",
    "        self.order_products_train = pd.read_csv(self.path_to_order_products_train)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        '''\n",
    "        создает таблицу user-item для дальнейших манипуляций\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.merged = pd.merge(self.orders, self.order_products_train, on='order_id', how='right')\n",
    "        self.merged = self.merged[['order_id', 'user_id', 'product_id']]\n",
    "        \n",
    "        self.user_id_num = {key: value for value, key in enumerate(self.merged['user_id'].unique(), start=0)}\n",
    "        self.product_id_num = {key: value for value, key in enumerate(self.merged['product_id'].unique(), start=0)}\n",
    "        self.product_num_id = {value: key for value, key in enumerate(self.merged['product_id'].unique(), start=0)}\n",
    "        \n",
    "        self.merged['user_num'] = self.merged['user_id'].transform(lambda x: self.user_id_num[x])\n",
    "        self.merged['product_num'] = self.merged['product_id'].transform(lambda x: self.product_id_num[x])\n",
    "        self.merged['buy'] = 1\n",
    "        \n",
    "        self.user_item_matrix = sps.coo_matrix((self.merged.buy, (self.merged.user_num, self.merged.product_num)))\n",
    "        \n",
    "    def decompose(self):\n",
    "        '''\n",
    "        декомпозиция user-item_matrix с размерностью n_comp\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.model = NMF(n_components=self.n_comp, init='random', random_state=0)\n",
    "        self.user_matrix = self.model.fit_transform(self.user_item_matrix)\n",
    "        self.product_matrix = self.model.components_\n",
    "    \n",
    "    def get_similarity(self, top_n_size):\n",
    "        '''Каждому юзеру сопоставляет id n-схожих юзеров'''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.similar_users_df = pd.DataFrame(columns=[i for i in range(1, top_n_size + 1)])\n",
    "\n",
    "        for user_num in tqdm(range(len(self.user_matrix))):\n",
    "          user_to_others_similarity = cosine_similarity(X=self.user_matrix, Y=self.user_matrix[user_num].reshape(1, self.n_comp))\n",
    "          top_n = pd.DataFrame(user_to_others_similarity).sort_values(by=0, ascending = False)[:top_n_size].index\n",
    "          self.similar_users_df.loc[user_num] = top_n\n",
    "    \n",
    "    def save_similarity_df(self, path):\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.similar_users_df.to_csv(path)\n",
    "        \n",
    "    def set_similarity_df(self, path_to_df):\n",
    "        '''\n",
    "        Если уже есть посчитанная матрица, то можем использовать и обойти долгий кусок обучения в fit(),\n",
    "        указав параметр loaded_df=True\n",
    "        '''\n",
    "        if not os.path.isfile(path_to_df):\n",
    "            self.ok_status = False\n",
    "            return 'File doesnt exist'\n",
    "        \n",
    "        self.similar_users_df = pd.read_csv(path_to_df)\n",
    "    \n",
    "    def get_predicted_user_item_matrix(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, path_to_save=False, loaded_df=False, top_n_size=5):\n",
    "        '''\n",
    "        :param self:\n",
    "        :param path_to_save: путь для сохранения матрицы схожести\n",
    "        :param loaded_df: bool - True если матрица уже была загружена и пересчитывать заново ее не нужно;\n",
    "                          default=False\n",
    "        :param top_n_size: размер топ схожих пользователей \n",
    "        :return: информация в случае ошибки \n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        self.read_data()     # читает необходимые df\n",
    "        self.prepare_data()  # создание таблицы user-item с необх. полями и разметками\n",
    "        self.decompose()     # декомпозиция матрицы на 2 матрицы размерности n\n",
    "        if not loaded_df:\n",
    "            self.get_similarity(top_n_size)\n",
    "            if path_to_save:\n",
    "                self.save_similarity_df(path_to_save)\n",
    "        \n",
    "    def predict(self, user_id):\n",
    "        '''\n",
    "        :param self:\n",
    "        :param user_id:  id юзера, для которого считаем предсказание покупок\n",
    "        :return: сообщение в случае ошибки; в случае успеха - словарь {product_id: сумма весов}\n",
    "        '''\n",
    "        if not self.ok_status:\n",
    "            return 'There were some problems'\n",
    "        \n",
    "        if user_id in self.user_id_num:\n",
    "            user_num = self.get_user_num(user_id)\n",
    "        else:\n",
    "            return 'There is no user with this id'\n",
    "        \n",
    "        # индексы n_comp схожих пользователей\n",
    "        sim_users_nums = self.similar_users_df.loc[user_num].values\n",
    "        d = defaultdict(int)\n",
    "        for sim_user_num in sim_users_nums:\n",
    "            sim_user_purchases = pd.Series.sparse.from_coo(self.user_item_matrix)[sim_user_num].index\n",
    "            sim_user_weight = cosine_similarity(X=self.user_matrix[sim_user_num].reshape(1, self.n_comp), Y=self.user_matrix[user_num].reshape(1, self.n_comp))\n",
    "            \n",
    "            for product in sim_user_purchases:\n",
    "                product_id = self.product_num_id[product]\n",
    "                d[product_id] += sim_user_weight\n",
    "        \n",
    "        return sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def get_coo(self):\n",
    "        return self.user_item_matrix\n",
    "    \n",
    "    def get_user_num(self, user_id):\n",
    "        return self.user_id_num[user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример работы с обучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# файл, куда по необходимости загрузим результаты\n",
    "path_to_res = 'data/results.csv'\n",
    "\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_path = 'data_split/train_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 10\n",
    "\n",
    "# top_n\n",
    "top_n_size = 5\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_path, n_comp)\n",
    "\n",
    "# Обучение с параметром path_to_save=filename после рассчета матрицы схожести сохранит df по указанному пути\n",
    "# обучение длится долго, так что крайне советую этот параметр указать\n",
    "Recommender.fit(path_to_save=path_to_res, top_n_size=top_n_size)\n",
    "\n",
    "# можем пресказать что-то для юзера с id=12345 (пример выводы в следующей ячейке, так как матрица у меня уже была посчитана)\n",
    "Recommender.predict(125)\n",
    "\n",
    "# результат - номер продукта и пока просто сумма весов (те никак не нормализованное число, посчитано по матрице с 0/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример работы без обучения (то есть уже есть просчитанная матрица - загрузим ее и сделаем предсказание):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29487, array([[4.99052653]])),\n",
       " (19348, array([[2.99291994]])),\n",
       " (39646, array([[2.]])),\n",
       " (11827, array([[1.]])),\n",
       " (27676, array([[1.]])),\n",
       " (31564, array([[1.]])),\n",
       " (47502, array([[1.]])),\n",
       " (42284, array([[1.]])),\n",
       " (12620, array([[1.]])),\n",
       " (5460, array([[1.]])),\n",
       " (13866, array([[1.]])),\n",
       " (26965, array([[1.]])),\n",
       " (7877, array([[1.]])),\n",
       " (43056, array([[1.]])),\n",
       " (10613, array([[1.]])),\n",
       " (9755, array([[1.]])),\n",
       " (34726, array([[1.]])),\n",
       " (37543, array([[1.]])),\n",
       " (13287, array([[1.]])),\n",
       " (29176, array([[1.]])),\n",
       " (20463, array([[1.]])),\n",
       " (49079, array([[0.99796744]])),\n",
       " (38279, array([[0.99796744]])),\n",
       " (12911, array([[0.99796744]])),\n",
       " (18681, array([[0.99796744]])),\n",
       " (39282, array([[0.99796744]])),\n",
       " (48927, array([[0.99761497]])),\n",
       " (9387, array([[0.99760658]])),\n",
       " (41591, array([[0.99760658]])),\n",
       " (7736, array([[0.99760658]])),\n",
       " (432, array([[0.99760658]])),\n",
       " (13921, array([[0.99760658]])),\n",
       " (45835, array([[0.99760658]])),\n",
       " (39411, array([[0.99760658]])),\n",
       " (42441, array([[0.99760658]])),\n",
       " (27696, array([[0.99760658]])),\n",
       " (27850, array([[0.99760658]])),\n",
       " (42509, array([[0.99760658]])),\n",
       " (36931, array([[0.99760658]])),\n",
       " (23296, array([[0.99733754]])),\n",
       " (35613, array([[0.99733754]])),\n",
       " (46817, array([[0.99733754]])),\n",
       " (15923, array([[0.99733754]])),\n",
       " (6680, array([[0.99733754]]))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# файл, где лежат результаты\n",
    "path_to_res = 'data/results.csv'\n",
    "\n",
    "# необходимые файлы: \n",
    "# 1. исходный из keggle orders\n",
    "path_to_orders = 'data/orders.csv'\n",
    "\n",
    "# 2. созданный-смерженный как надо методами из п.II\n",
    "df_train_path = 'data_split/train_data.csv'\n",
    "\n",
    "# размерность для декомпозиции\n",
    "n_comp = 10\n",
    "\n",
    "# создаем экземпляр класса с вышеописанными параметрами\n",
    "Recommender = UserBasedRecommender(path_to_orders, df_train_path, n_comp)\n",
    "\n",
    "# Загружаем уже посчитанные результаты\n",
    "Recommender.set_similarity_df(path_to_res)\n",
    "\n",
    "# Обучение включает с параметром loaded_df=True создаст только необходимые для пресказания матрицы, \n",
    "# не относящиеся к уже имеющимся результатам (по дефолту False и для крохотных размерностей считалось 3 часа...)\n",
    "Recommender.fit(loaded_df=True)\n",
    "\n",
    "# можем пресказать что-то для юзера с id=125\n",
    "Recommender.predict(125)\n",
    "\n",
    "# результат - номер продукта и пока просто сумма весов (те никак не нормализованное число, посчитано по матрице с 0/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Пропишем все пути и сконструируем датасеты мечты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# необходимые файлы с исходными датасетами с keggle\n",
    "path_data_train = 'D:/Projects/Anaconda/EDA/data/order_products__train.csv'\n",
    "path_data_prior = 'D:/Projects/Anaconda/EDA/data/order_products__prior.csv'\n",
    "path_data_orders = 'D:/Projects/Anaconda/EDA/data/orders.csv'\n",
    "    \n",
    "# [куда запишем]/[где уже лежат] файлы с индексами\n",
    "train_path = 'data_split/train_indices.pickle'\n",
    "valid_path = 'data_split/validation_indices.pickle'\n",
    "test_path = 'data_split/test_indices.pickle'\n",
    "\n",
    "# куда запишем файлы с новым разделением\n",
    "df_train_path = 'data_split/train_data.csv'\n",
    "df_valid_path = 'data_split/validation_data.csv'\n",
    "df_test_path = 'data_split/test_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Если файлов с индексами нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_indices(grouped_ratings, retriever):\n",
    "    return np.concatenate(grouped_ratings.apply(retriever).values)\n",
    "\n",
    "def split(path_to_orders):\n",
    "    orders = pd.read_csv(path_to_orders)\n",
    "    grouper = orders.sort_values('order_number').groupby('user_id')\n",
    "    train_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings[:int(user_ratings.shape[0] * 0.5)].index.values)\n",
    "    \n",
    "    validation_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings.iloc[int(user_ratings.shape[0] * 0.5):\n",
    "                                               int(user_ratings.shape[0] * 0.75)].index.values)\n",
    "    \n",
    "    test_indices = _split_indices(\n",
    "        grouper,\n",
    "        lambda user_ratings: user_ratings.iloc[int(user_ratings.shape[0] * 0.75):].index.values)\n",
    "    \n",
    "    return train_indices, validation_indices, test_indices\n",
    "\n",
    "\n",
    "train_indices, validation_indices, test_indices = split(path_data_orders)\n",
    "\n",
    "# save results\n",
    "with open(train_path, 'wb') as out:\n",
    "    pickle.dump(train_indices, out)\n",
    "\n",
    "with open(valid_path, 'wb') as out:\n",
    "    pickle.dump(validation_indices, out)\n",
    "\n",
    "with open(test_path, 'wb') as out:\n",
    "    pickle.dump(test_indices, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файлы с индексами готовы - теперь скомпонуем и запишем в файл все необходимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concat_same_data(df_train_path, df_prior_path, df_orders_path):\n",
    "    \n",
    "    df_train = pd.read_csv(df_train_path)\n",
    "    df_prior = pd.read_csv(df_prior_path)\n",
    "    df_orders = pd.read_csv(df_orders_path)\n",
    "    \n",
    "    order_products_full = pd.concat([df_train, df_prior])\n",
    "    \n",
    "    return order_products_full, df_orders\n",
    "\n",
    "\n",
    "def get_split_df(train_path, valid_path, test_path, df_orders, df_order_products_full):\n",
    "    \n",
    "    with open(train_path, 'rb') as input:\n",
    "        train_indices = pickle.load(input)\n",
    "\n",
    "    with open(valid_path, 'rb') as input:\n",
    "        validation_indices = pickle.load(input)\n",
    "\n",
    "    with open(test_path, 'rb') as input:\n",
    "        test_indices = pickle.load(input)\n",
    "        \n",
    "    train_order_ids = df_orders.loc[train_indices]['order_id'].values\n",
    "    validation_order_ids = df_orders.loc[validation_indices]['order_id'].values\n",
    "    test_order_ids = df_orders.loc[test_indices]['order_id'].values\n",
    "    \n",
    "    train_df = df_order_products_full.loc[train_order_ids]\n",
    "    validation_df = df_order_products_full.loc[validation_order_ids]\n",
    "    test_df = df_order_products_full.loc[test_order_ids]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "# считается что уже имеем 3 файла с индексами\n",
    "df_order_products_full, df_orders = read_and_concat_same_data(path_data_train, path_data_prior, path_data_orders)\n",
    "train_df, validation_df, test_df = get_split_df(train_path, valid_path, test_path, df_orders, df_order_products_full)\n",
    "\n",
    "# write new df's to file:\n",
    "train_df.to_csv(df_train_path, index=False)\n",
    "validation_df.to_csv(df_valid_path, index=False)\n",
    "test_df.to_csv(df_test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лучше хранить в словаре, так как могут быть разные n для разных юзеров (как переделать текущий дф):\n",
    "# d = df.to_dict('index')\n",
    "# d[user_num].values() - выдает словарь список - можно пройтись в цикле и переписать\n",
    "\n",
    "# в классе заюзаем dict и просто впишем массив, подфиксить под это надо predict(), fit(), set_similarity_df()\n",
    "\n",
    "# учесть то, что для разных юзеров разное n:\n",
    "# добавть параметр if sim<min_sim: отбрасываем юзера из топа => в таком случае n будет меньше стандартного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pd.DataFrame(columns=[i for i in range(1, 3)])\n",
    "d = h.to_dict('index')\n",
    "for num in d[3].values():\n",
    "    print(num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
